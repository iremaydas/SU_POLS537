
#Part I: Probability Theory and Bayes Theorem
*Question.1

a)The marginal distributions of X and Y

The marginal distributions of X for all x is below:

\begin{array} {|r|r|}\hline X     &     0 &     1 \\ \hline P(X=x)     &     0.45     &      0.55 \\ \hline  \end{array}

The marginal distributions of Y for all y is below:

\begin{array} {|r|r|}\hline Y & 1 & 2 & 3 \\ \hline P(Y=y) & 0.20 & 0.35 & 0.45 \\ \hline  \end{array}

b)The conditional distribution of Y given that X = 1 

\begin{array} {|r|r|}\hline Y & 1 & 2 & 3 \\ \hline P(X=1|Y=y) & 0.27 & 0.18 & 0.54 \\ \hline  \end{array}


c)Expected Values of X and Y

$E(X)=(0$\times$(0.45))+(1$\times$(0.55))$
    $=0.55$

$E(Y)=(1$\times$(0.20)+(2$\times$(0.35)+3$\times$(0.45))$
    $=2.25$

d)Expected value of XY

$E(XY)=((0)$\times$(1)$\times$(0.05))+((0)$\times$(2)$\times$(0.25))+((0)$\times$(3)$\times$(0.15))+((1)$\times$(1)$\times$(0.15))+((1)$\times$(2)$\times$(0.10))+((1)$\times$(3)$\times$(0.30))$
   $=1.25$

e)Interdependence

X and Y are independent if and only if $P(x,y)$=$P(y \mid x)$\times$P(x)$=$p(y)$\times$p(x)$

For instance, the following equation should be working. 

$P(x=0,y=1)$=$P(y=1 \mid x=0)$\times$P(x=0)$=$p(y=1)$\times$p(x=0)$

0.05$\neq$(0.45)$\times$(0.20)

Therefore, X and Y are not interdependent.


*Question.2

$P(A \mid B)$ = $P(B \mid A)$ $\times$ $P(A)$ $/$ $P(B)$

=(0.7 x 0.05 ) / (0.7 x 0.05) + (0.1 x  0.95)


#Part II: Naive Bayes
```{r, echo=TRUE}
## Loading the data of the web
rawData=read.csv("https://raw.githubusercontent.com/babakrezaee/MethodsCourses/master/DataSets/MiddleEast_2016-2019_Mar16.csv",
stringsAsFactors=FALSE,encoding="UTF-16")

## Decreasing sample size for efficiency 
mysample <- rawData[sample(1:nrow(rawData), 5000, replace=FALSE),]

# Let's take a look at the first few observations
head(mysample)

mysample[1:5,]
```
```{r, echo=TRUE}
library(knitr)
library(kableExtra)
kable(head(mysample), format='latex')
```
```{r, echo=TRUE}
library(xtable)
print(xtable(head(mysample),align = c("l", "l", "l")))
```
```{r mysample$EVENT_TYPE}
summary(mysample$EVENT_TYPE)
```
```{r, echo=TRUE}
is.factor(mysample$EVENT_TYPE)
```
```{r, echo=TRUE}
mysample$EVENT_TYPE=factor(mysample$EVENT_TYPE)
plot(mysample$EVENT_TYPE, las=2)

```
```{r, echo=TRUE}
library(ggplot2) 
ggplot(rawData, aes(EVENT_TYPE))+
geom_bar()+
theme(axis.text.x = element_text(angle = 60, hjust = 1))

##Word cloud shows the most frequents words

library(wordcloud)
wordcloud(mysample$NOTES, min.freq = 3,
max.words=200, random.order=FALSE, rot.per=0,
colors=brewer.pal(8, "Dark2"))
```

##Generating 2 general levels as violent and non-violent
```{r, echo=TRUE}
levels(mysample$EVENT_TYPE)
levels(mysample$EVENT_TYPE)[levels(mysample$EVENT_TYPE)=="Battles"]="Violent"
levels(mysample$EVENT_TYPE)[levels(mysample$EVENT_TYPE)=="Explosions/Remote violence"]="Violent"
levels(mysample$EVENT_TYPE)[levels(mysample$EVENT_TYPE)=="Violence against civilians"]="Violent"
levels(mysample$EVENT_TYPE)[levels(mysample$EVENT_TYPE)=="Protests"]="Non-violent"
levels(mysample$EVENT_TYPE)[levels(mysample$EVENT_TYPE)=="Riots"]="Non-violent"
levels(mysample$EVENT_TYPE)[levels(mysample$EVENT_TYPE)=="Strategic developments"]="Non-violent"
```
```{r, eval=FALSE}
ggplot(mysample, aes(EVENT_TYPE))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```
```{r, echo=TRUE}
library(tm)
library(SnowballC)
Encoding(mysample$NOTES)  <- "UTF-8"

##Generating corpus of texts and cleaning up
myCorpus=VCorpus(VectorSource(mysample$NOTES))
myCl=tm_map(myCorpus, content_transformer(stringi::stri_trans_tolower))
myCl=tm_map(myCorpus,content_transformer(removePunctuation))
myCl=tm_map(myCl, stemDocument) #stem = root
myCl=tm_map(myCl, stripWhitespace)
myCl[[1]]$conten
myCl[[1]]

##For creating data structure
myDTM=DocumentTermMatrix(myCl)

##Finding words that occur at least 10 times
```
```{r, eval=FALSE}
findFreqTerms(myDTM, lowfreq = 10)
```
```{r, echo=TRUE}
##Data Preparation
myTrain=myDTM[1000:4000,]
myTest=myDTM[4001:5000,]

#Outcome variable is event type(Violent vs Non-violent)
myTrainY=mysample[1000:4000,]$EVENT_TYPE 
myTestY=mysample[4001:5000,]$EVENT_TYPE
```
```{r, echo=TRUE}
library(e1071)

myFreqWords=findFreqTerms(myTrain, 5) ##5 refers to the words that occured min 5 times
myFreqTrain=myTrain[,myFreqWords]
myFreqTest=myTest[,myFreqWords]

##The Naive Bayes works on categorical features. HOwever, we have a matrix with numbers. So, we need to change this.
convertCounts <-function(x){
  x<-ifelse(x>0, "Yes", "No")
}

myTrain=apply(myFreqTrain, MARGIN = 2, convertCounts)
myTest=apply(myFreqTest,MARGIN = 2, convertCounts)
dim(myTrain)
myNB=naiveBayes(myTrain, myTrainY, laplace = 1)
yhat=predict(myNB,myTest)
```
```{r, eval=FALSE}
ctab=table(yhat,myTestY)
ctab
```
```{r, eval=FALSE}
missClass=1-(sum(ctab)-sum(diag(ctab)))/sum(ctab)

perNon=ctab[2,2]/sum(ctab[,2])
cat("Missclassification and Non-violent resistance classification rate",round(missClass,2),"% and ",round(perNon,2),"%, respectively \n")

#Missclassification rate will be same. 
perViolent=ctab[1,1]/sum(ctab[,1])
cat("Missclassification and Violent resistance classification rate",round(missClass,2),"% and ",round(perViolent,2),"%, respectively \n")
```
Missclassification and Non-violent conflict classification rate 0.96 % and  0.96 %, respectively 
Missclassification and Violent conflict classification rate 0.96 % and  0.97 %, respectively 


**Laplace Parameter
##If we do not use Laplace parameter, the probability of occurence of a new word will be 0. Laplace smoothing gives the "new word" a non-0 probability and decreases the information loses.

##First, I applied naive bayes classification with laplace 1.
```{r, eval=FALSE}
NB1=naiveBayes(myTrain,myTrainY, laplace=1)

yhat1=predict(NB1,myTest)
ctab1=table(yhat1,myTestY)
ctab1

missClass1=1-(sum(ctab1)-sum(diag(ctab1)))/sum(ctab1)

cat("Missclassification rate",round(missClass1,2))
```
##Then I tries for different laplace values from 0.25 to 2 by 0.25 increases.

```{r, eval=FALSE}
NB2=naiveBayes(myTrain,myTrainY, laplace=0.25)
yhat2=predict(NB2,myTest)
ctab2=table(yhat2,myTestY)
ctab2

missClass2=1-(sum(ctab2)-sum(diag(ctab2)))/sum(ctab2)
cat("Missclassification rate 2",round(missClass2,2))

NB3=naiveBayes(myTrain,myTrainY, laplace=0.50)
yhat3=predict(NB3,myTest)
ctab3=table(yhat3,myTestY)
ctab3

missClass3=1-(sum(ctab3)-sum(diag(ctab3)))/sum(ctab3)
cat("Missclassification and Conflict classification rate 3",round(missClass3,2))

NB4=naiveBayes(myTrain,myTrainY, laplace=0.75)
yhat4=predict(NB4,myTest)
ctab4=table(yhat4,myTestY)
ctab4

missClass4=1-(sum(ctab4)-sum(diag(ctab4)))/sum(ctab4)

cat("Missclassification rate 4",round(missClass4,2))


NB5=naiveBayes(myTrain,myTrainY, laplace=1.25)

yhat5=predict(NB5,myTest)
ctab5=table(yhat5,myTestY)
ctab5

missClass5=1-(sum(ctab5)-sum(diag(ctab5)))/sum(ctab5)
cat("Missclassification rate 5",round(missClass5,2))

NB6=naiveBayes(myTrain,myTrainY, laplace=1.50)
yhat6=predict(NB6,myTest)
ctab6=table(yhat6,myTestY)
ctab6
missClass6=1-(sum(ctab6)-sum(diag(ctab6)))/sum(ctab6)

cat("Missclassification rate 6",round(missClass6,2))

NB7=naiveBayes(myTrain,myTrainY, laplace=1.75)
yhat7=predict(NB7,myTest)
ctab7=table(yhat7,myTestY)
ctab7
missClass7=1-(sum(ctab7)-sum(diag(ctab7)))/sum(ctab7)
cat("Missclassification rate 7",round(missClass7,2))

NB8=naiveBayes(myTrain,myTrainY, laplace=2)
yhat8=predict(NB8,myTest)
ctab8=table(yhat8,myTestY)
ctab8
missClass8=1-(sum(ctab8)-sum(diag(ctab8)))/sum(ctab8)

cat("Missclassification rate 8",round(missClass8,2))
```

*Plotting different laplace values and missclassification rates
```{r, echo=TRUE}
mylaplace = c(0.25, 0.50, 0.75, 1, 1.25, 1.50, 1.75, 2)
mylaplaceMC = c(0.95, 0.96, 0.965, 0.965, 0.965, 0.965, 0.965, 0.965)
plot(mylaplace, mylaplaceMC)
```

##Laplace = 1 gives the most optimal.

###Part III: kNN Algorithm
```{r, echo=TRUE}
Navco1=read.csv("https://raw.githubusercontent.com/babakrezaee/MethodsCourses/aaf06fea14c96eca902c0c0af81ad86e0ff0ba03/DataSets/NAVCO1.csv")
attach(Navco1)
## The scatter plot shows how the duration(log) of resistance campaigns and regime type are associated:
plot(tpolity,lduration,
xlab='Regime type',ylab = 'Duration (log)',
main = "Calculating KNN using different values of k",
pch=19, col="maroon")

lmduration=lm(lduration~tpolity)
abline(lmduration$coef, col="navy", lwd=2.5)
##The relationship between these variables are not linear as scatter plot and blue line indicates. Therefore, employing a parametric model would not capture all the information.

yhat_lm25=predict(lmduration,data.frame(tpolity=c(0)))
points(-3,yhat_lm25,cex=1, col="grey10", pch=15)
text(-3,yhat_lm25,paste(round(yhat_lm25,digits = 2)),pos=3, cex=1,col="grey10")
##The duration(log) of resistance campaings in countries with regime type score '-3' is 6.6.

```
```{r, echo=TRUE}
library(kknn) 
n = nrow(Navco1) 
train = data.frame(tpolity,lduration) 
test = data.frame(tpolity = sort(tpolity))

K=c(3,10,15,20,50)

plot(tpolity,lduration,
     xlab='regime',ylab = 'duration',
     main = "Calculating KNN using different values of k",
     pch=19, col="gray45")

for (i in K){
  nam <- paste("KF", i, sep = "_")
  assign(nam,kknn(lduration~tpolity,train,test,k=i,kernel = "rectangular"))
}

lines(test$tpolity,KF_3$fitted.values,col="darkred",lwd=3)
lines(test$tpolity,KF_10$fitted.values,col="blue",lwd=3)
lines(test$tpolity,KF_15$fitted.values,col="violet",lwd=3)
lines(test$tpolity,KF_20$fitted.values,col="green",lwd=3)
lines(test$tpolity,KF_50$fitted.values,col= "cyan",lwd=3)

legend("topright",legend=c("K=3","K=10","K=15","K=20","K=50"),
       col=c("darkred","blue","violet","green","cyan"),
       lty=1, box.lty=0, cex=0.6)
       
#For k=20, the plot looks nice since it not sensitive as much as smaller values and also it tries to capture more information than bigger values.
dfp = data.frame(tpolity=-3)
kf20_3 = kknn(lduration~tpolity,train,dfp,k=20,kernel = "rectangular")
cat("kNN20: Predicted price of a resistance movement that is campaigning against a government with democracy index=-3 is",round(kf20_3$fitted,digits=2),"\n")

#### Cross-Validation kknn4
```{r, echo=TRUE}
Navco1=read.csv("https://raw.githubusercontent.com/babakrezaee/MethodsCourses/aaf06fea14c96eca902c0c0af81ad86e0ff0ba03/DataSets/NAVCO1.csv")
data = data.frame(Navco1$lduration,Navco1$tpolity)

data=na.omit(data) 
# dat=data[!rowSums((is.na(data))),]
y=cbind(data$Navco1.lduration)

y=as.vector(y)

library(kknn)
library(devtools)
source_url('https://raw.githubusercontent.com/babakrezaee/DataWrangling/master/Functions/docv_fun.R') ## Cross validation function

set.seed(1234)

kv=2:200 

ndocv=35 

cv_mat_5=matrix(0,length(kv),ndocv)

for (i in 1:ndocv){
  cv_temp=docvknn(data.frame(data$Navco1.tpolity),y,kv,nfold=5)
  cv_mat_5[,i]=sqrt(cv_temp/length(y))
}
cv_mean_1=apply(cv_mat_5,1,mean)
kbest_1 = kv[which.min(cv_mean_1)]
cat("The min value of RMSE is associated with k=",kbest_1,"\n")
kbest_1 

plot(kv, cv_mean_1, xlab="K", ylab="RMSE", type='l', col="black", lwd=2 )
for (i in 1:ndocv) lines(kv,cv_mat_5[,i], col=550+i, lwd=.4)
lines(kv, cv_mean_1, xlab="K", ylab="RMSE", col="black", lwd=2, lty=2 )
title(main="nfold=5", font.main= 1)

```
#I picked k=20 with eye-ball method
```{r, echo=TRUE}
kf20 = kknn(lduration~tpolity,train,test,k=20,kernel = "rectangular")

kf20_0 = kknn(lduration~tpolity,train,dfp,k=20,kernel = "rectangular")
cat("kNN20: mlvs",round(kf20_0$fitted,digits=20),"\n")

plot(tpolity,lduration,
     xlab='regime',ylab = 'duration',
     main = "Calculating KNN using different values of k",
     pch=19, col="gray45")
lines(test$tpolity,KF_20$fitted.values,col="darkred",lwd=3)

#K=97 is the best k value that gives the lowest RMSE
k=c(97)
for (i in k){
  nam <- paste("KF", i, sep = "_")
  assign(nam,kknn(lduration~tpolity,train,test,k=i,kernel = "rectangular"))
}

lines(test$tpolity,KF_97$fitted.values,col="blue",lwd=3)

legend("topright",legend=c("K=2","K=102"),
       col=c("darkred","blue"),
       lty=1, box.lty=0, cex=0.6)


kf97 = kknn(lduration~tpolity,train,test,k=97,kernel = "rectangular")
kf97_0 = kknn(lduration~tpolity,train,dfp,k=97,kernel = "rectangular")
cat("kNN97: mlvs",round(kf97_0$fitted,digits=2),"\n")
##Comparing k=97 and k=20 for regimes with democracy index=-3
points(-3,kf97_0$fitted,cex=1, col="maroon", pch=15)
text(-3,kf97_0$fitted,paste(round(kf97_0$fitted,digits = 2)),pos=3, cex=1,col="black")  

points(-3,kf20_0$fitted,cex=1, col="maroon", pch=15)
text(-3,kf20_0$fitted,paste(round(kf20_0$fitted,digits = 2)),pos=3, cex=1,col="black") 
```
##Multiple Features
```{r, eval=FALSE}
Navco1=read.csv("https://raw.githubusercontent.com/babakrezaee/MethodsCourses/aaf06fea14c96eca902c0c0af81ad86e0ff0ba03/DataSets/NAVCO1.csv")

data = data.frame(Navco1$lduration,Navco1$tpolity,Navco1$lmembers,Navco1$rgdppc,Navco1$growthrgdppc,Navco1$lupop,Navco1$lmtnest,Navco1$ef,Navco1$tgovcap)
data=na.omit(data) 
attach(data$Navco1)
y=cbind(data$Navco1.lduration)

y=as.vector(y)

library(kknn)
library(devtools)
source_url('https://raw.githubusercontent.com/babakrezaee/DataWrangling/master/Functions/docv_fun.R') 

X=cbind(data$Navco1.tpolity,data$Navco1.lmembers,data$Navco1.rgdppc,data$Navco1.growthrgdppc,data$Navco1.lupop,data$Navco1.lmtnest,data$Navco1.ef,data$Navco1.tgovcap)
##Standardization of features to have equal range
mmsc=function(x){return((x-min(x))/(max(x)-min(x)))}
X_s=apply(X, 2, mmsc)

set.seed(1234)

kv=2:180

ndocv=35 

cv_mat_5=matrix(0,length(kv),ndocv)

for (i in 1:ndocv){
  cv_temp=docvknn(X_s,y,kv,nfold=5)
  cv_mat_5[,i]=sqrt(cv_temp/length(y))
}  

for (i in 1:ndocv){
  cv_temp=docvknn(X_s,y,kv,nfold=5)
  cv_mat_5[,i]=sqrt(cv_temp/length(y))
}  

cv_temp=docvknn(X_s,y,kv,nfold=5)
cv_mat_5[,i]=sqrt(cv_temp/length(y))

cv_mean_1=apply(cv_mat_5,1,mean)
kbest_1 = kv[which.min(cv_mean_1)]
cat("The min value of RMSE is associated with k=",kbest_1,"\n")
kbest_1 

plot(kv, cv_mean_5, xlab="K", ylab="RMSE", type='l', col="black", lwd=2 )
for (i in 1:ndocv) lines(kv,cv_mat_5[,i], col=550+i, lwd=.4)
lines(kv, cv_mean_5, xlab="K", ylab="RMSE", col="black", lwd=2, lty=2 )
title(main="nfold=5", font.main= 1)
```
##Comparison of Kernel Options
```{r, eval=FALSE}
##The best kernel is rectangular because this kernel has lower mean squared error.
(train.ord <- train.kknn(data$Navco1.lduration ~ data$Navco1.tpolity + data$Navco1.lmembers +data$Navco1.rgdppc + data$Navco1.growthrgdppc +data$Navco1.lupop +data$Navco1.lmtnest+data$Navco1.ef +data$Navco1.tgovcap, data$Navco1, kmax = 25,
                         kernel = c("rectangular", "triangular", "epanechnikov", "gaussian",
                                    "rank", "optimal")))

plot(train.ord)
```
